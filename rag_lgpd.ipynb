{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2:3b\",\n",
    "    temperature = 0,\n",
    "    base_url='http://localhost:11434'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração de Histórico e Rastreabilidade de Consultas usando LLMs\n",
    "\n",
    "## Objetivo\n",
    "Neste notebook, exploraremos os impactos do uso de histórico e rastreabilidade em consultas realizadas por meio de um modelo de linguagem grande (LLM). Utilizaremos metadados associados às interações para fornecer contexto e rastreabilidade, além de metadados gerados pelo próprio modelo para análise.\n",
    "\n",
    "## Modelo Utilizado\n",
    "Usaremos o modelo **Llama3.2:3B** como base, acessado via **Ollama**, uma ferramenta que facilita a integração e execução de modelos LLM localmente.\n",
    "\n",
    "## Metodologia\n",
    "1. **Histórico de Consultas**: Manteremos um log estruturado de todas as interações realizadas com o modelo.\n",
    "2. **Rastreabilidade via Metadados**: Adicionaremos informações contextuais como usuário, data, e propósito da consulta, permitindo uma análise detalhada e rastreamento das interações.\n",
    "3. **Metadados Gerados pelo Modelo**: Exploraremos como os metadados criados pelo modelo podem complementar o processo de rastreabilidade e análise.\n",
    "\n",
    "\n",
    "## Resultado Esperado\n",
    "- Análise do impacto da rastreabilidade nas consultas.\n",
    "- Organização do histórico para facilitar a auditoria e reprodutibilidade.\n",
    "- Identificação de padrões nos metadados gerados pelo modelo.\n",
    "\n",
    "## Ferramentas e Recursos\n",
    "- **Modelo**: Llama3.2:3B.\n",
    "- **Framework**: Ollama.\n",
    "- **Linguagem**: Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 1: Pergunta Inicial ao LLM\n",
    "\n",
    "Nesta etapa, faremos uma consulta direta ao modelo de linguagem (LLM) sem fornecer informações adicionais ou contextos baseados em documentos. O objetivo é avaliar a capacidade do modelo de responder à pergunta com base em seu treinamento e conhecimento prévio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. Tente reiniciar o Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNão é possível executar o código, a sessão foi descartada. Tente reiniciar o Kernel. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Qual capitulo da lgpd trata do tratamento de dados pessoais pelo poder publico?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Correção e Análise\n",
    "O modelo não forneceu a resposta correta para a pergunta. De acordo com a Lei Geral de Proteção de Dados (LGPD), o capítulo que trata do tratamento de dados pessoais pelo poder público é o **Capítulo IV**, abrangendo os artigos que detalham as bases legais e os princípios aplicáveis ao tratamento de dados pelo Poder Público.\n",
    "\n",
    "Agora, para melhorar o contexto e a precisão da resposta, iremos consumir o conteúdo do **PDF da LGPD** e enviá-lo como parte do contexto para a próxima consulta ao modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    extracted_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "\n",
    "            if page.extract_text():\n",
    "                extracted_text.append(page.extract_text())\n",
    "            else:\n",
    "                page_image = page.to_image(resolution=300)\n",
    "                image = page_image.original  \n",
    "                text = image_to_string(image)\n",
    "                extracted_text.append(text)\n",
    "    return extracted_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "O objetivo desta etapa é extrair o texto completo do **PDF da LGPD** para que possamos usá-lo como contexto na próxima consulta ao modelo. \n",
    "\n",
    "## Método Utilizado\n",
    "1. **PDFPlumber**: Usado para extrair texto diretamente de páginas do PDF.\n",
    "2. **OCR (Reconhecimento Óptico de Caracteres)**: Caso o texto de uma página não seja extraído diretamente (por exemplo, devido a imagens), aplicamos o OCR usando o `pytesseract`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/lgpd.pdf\"\n",
    "text = extract_text_with_ocr(pdf_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Integração com LangChain, FAISS e Modelo Ollama\n",
    "\n",
    "## Objetivo\n",
    "Este código implementa uma pipeline de **Retrieval-Augmented Generation (RAG)**, utilizando **LangChain**, **FAISS** e o modelo **Ollama**. O objetivo é fornecer respostas contextuais baseadas em documentos previamente indexados.\n",
    "\n",
    "## **1. LangChain**\n",
    "O LangChain é uma biblioteca projetada para facilitar a integração de modelos de linguagem com ferramentas externas, como bases de dados, vetores de busca e fluxos de trabalho complexos.\n",
    "\n",
    "### Componentes Utilizados:\n",
    "- **`langchain.schema.Document`**: Representa um documento estruturado com conteúdo textual que pode ser utilizado em pipelines de recuperação de informações.\n",
    "- **`langchain.schema.HumanMessage`**: Utilizado para encapsular mensagens enviadas por um usuário humano para interação com modelos de linguagem.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. HuggingFaceEmbeddings**\n",
    "Esta tecnologia é utilizada para converter textos em representações vetoriais (embeddings) que podem ser usadas para buscas semânticas e recuperação de informações.\n",
    "\n",
    "- **Modelo Utilizado**: `all-MiniLM-L6-v2`\n",
    "  - Este modelo, baseado no framework HuggingFace, é otimizado para gerar embeddings de alta qualidade em tarefas de similaridade semântica e recuperação de informações.\n",
    "- **Função**: Permitir a indexação de textos em um espaço vetorial, tornando possível a busca por documentos semelhantes com base em consultas.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. FAISS (Facebook AI Similarity Search)**\n",
    "FAISS é uma biblioteca altamente otimizada para busca de similaridade em grandes coleções de dados vetoriais.\n",
    "\n",
    "### Por que FAISS?\n",
    "- **Eficiência**: Projetado para realizar buscas rápidas e escaláveis em dados vetoriais.\n",
    "- **Integração**: Fácil integração com embeddings gerados pelo HuggingFace ou outros frameworks.\n",
    "- **Função no Código**:\n",
    "  - Armazena os embeddings dos documentos.\n",
    "  - Permite recuperar os documentos mais relevantes com base na similaridade entre o texto da consulta e os documentos indexados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document, HumanMessage\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "def create_documents(texts):\n",
    "    if isinstance(texts, list):\n",
    "        documents = [Document(page_content=text) for text in texts]\n",
    "    else:\n",
    "        documents = [Document(page_content=texts)]\n",
    "    return documents\n",
    "\n",
    "def create_faiss_vectorstore(documents):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def generate_response_with_ollama(vectorstore, query):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    print(\"Contexto Recuperado:\")\n",
    "    print(context[:200])\n",
    "    \n",
    "    prompt = f\"Contexto: {context}\\n\\nPergunta: {query}\\nResposta:\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content.strip()\n",
    "\n",
    "def rag_pipeline_with_ollama(texts, query):\n",
    "    documents = create_documents(texts)\n",
    "    vectorstore = create_faiss_vectorstore(documents)\n",
    "    response = generate_response_with_ollama(vectorstore, query)\n",
    "    return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Qual capítulo da LGPD trata do tratamento de dados pessoais pelo poder público?\"\n",
    "response = rag_pipeline_with_ollama(text, query)\n",
    "\n",
    "# Exibir resposta gerada\n",
    "print(\"\\nResposta Gerada:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Resposta\n",
    "Após incluir o texto da **Lei Geral de Proteção de Dados (LGPD)** como contexto, o modelo conseguiu gerar uma resposta **correta e precisa**. O **Capítulo IV** é, de fato, o capítulo da LGPD que trata do tratamento de dados pessoais pelo poder público.\n",
    "\n",
    "---\n",
    "\n",
    "## Observação\n",
    "Este resultado demonstra que o fornecimento de um **contexto relevante** (neste caso, o texto da lei) melhora significativamente a precisão das respostas geradas pelo modelo. Inicialmente, sem o contexto, a resposta era incorreta.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefício do Contexto Adicional\n",
    "A inclusão do texto da LGPD como contexto foi essencial para:\n",
    "1. **Proporcionar ao modelo informações diretas e relevantes**.\n",
    "2. **Garantir que a resposta fosse fundamentada em uma fonte confiável**.\n",
    "\n",
    "## Proximos passos será o de usar multiplos documentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from pytesseract import image_to_string\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(pdf_path):\n",
    "\n",
    "    extracted_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            if page.extract_text():\n",
    "                extracted_text.append(page.extract_text())\n",
    "            else:\n",
    "                page_image = page.to_image(resolution=300)\n",
    "                image = page_image.original\n",
    "                text = image_to_string(image, lang=\"por\")\n",
    "                extracted_text.append(text)\n",
    "    return \"\\n\".join(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_documents(pdf_paths):\n",
    "\n",
    "    all_documents = []\n",
    "    for path in pdf_paths:\n",
    "        print(f\"Carregando: {path}\")\n",
    "        try:\n",
    "            # Extract and consolidate text from the PDF\n",
    "            text = extract_text_with_ocr(path)\n",
    "            filename = os.path.basename(path)\n",
    "            document = Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": filename}  # Metadata includes the file name\n",
    "            )\n",
    "            all_documents.append(document)\n",
    "            print(f\"Documento consolidado para {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {path}: {e}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        raise ValueError(\"Nenhum documento foi carregado. Verifique os caminhos dos PDFs.\")\n",
    "    \n",
    "    print(f\"Total de documentos carregados: {len(all_documents)}\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(documents):\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, vectorstore):\n",
    "\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\" for doc in docs\n",
    "    ])\n",
    "    \n",
    "\n",
    "    prompt = f\"Contexto: {context}\\n\\nPergunta: {query}\\nResposta:\"\n",
    "    response = llm.invoke([prompt])\n",
    "    \n",
    "    output = {\n",
    "        \"question\": query,\n",
    "        \"context\": context[:500],\n",
    "        \"response\": response.content.strip(),\n",
    "        \"sources\": [doc.metadata.get(\"source\", \"Unknown\") for doc in docs]\n",
    "    }\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vectorstore(pdf_paths):\n",
    "\n",
    "    documents = load_pdf_documents(pdf_paths)\n",
    "    vectorstore = create_vectorstore(documents)\n",
    "    print(\"Vectorstore criado com sucesso.\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, vectorstore):\n",
    "\n",
    "    response = generate_response(query, vectorstore)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\"data/lgpd.pdf\", \"data/codigo_civil.pdf\"]\n",
    "vectorstore = prepare_vectorstore(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"definicao lei geral de protecao de dados\"\n",
    "response1 = ask_question(query1, vectorstore)\n",
    "print(\"Pergunta:\", response1[\"question\"])\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Contexto:\", response1[\"context\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response1[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"capitulo das perdas e danos?\"\n",
    "response2 = ask_question(query2, vectorstore)\n",
    "print(\"Pergunta:\", response2[\"question\"])\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response2[\"response\"])\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Contexto:\", response2[\"context\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response2[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observação Inicial\n",
    "Recuperamos as fontes utilizadas pelo sistema para responder a perguntas. No entanto, notamos que elas não estão **bem distribuídas**. \n",
    "\n",
    "### Problema Identificado\n",
    "Ao fazer perguntas específicas sobre a **LGPD**, o sistema consultou fontes não relacionadas, como o **Código Civil** (`codigo_civil.pdf`). Isso indica que:\n",
    "1. O sistema de recuperação de documentos está incluindo fontes irrelevantes.\n",
    "2. As respostas geradas podem ser influenciadas por documentos que não são pertinentes ao tema consultado.\n",
    "\n",
    "---\n",
    "\n",
    "## Solução Proposta: Uso de Chunks\n",
    "Para melhorar a precisão e relevância das fontes utilizadas, vamos implementar o **uso de chunks**. Essa abordagem permite dividir documentos em blocos menores, facilitando a recuperação de partes mais específicas e relevantes.\n",
    "\n",
    "### Benefícios do Uso de Chunks\n",
    "1. **Maior Precisão**: Reduz a probabilidade de incluir documentos irrelevantes ao restringir a busca a trechos específicos.\n",
    "2. **Contexto Focado**: Aumenta a relevância do contexto recuperado para responder a perguntas específicas.\n",
    "3. **Melhor Gerenciamento de Fontes**: Garante que as respostas sejam fundamentadas em documentos diretamente relacionados ao tema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap if end - overlap > start else end\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_documents_with_chunks(pdf_paths, chunk_size=1000, overlap=200):\n",
    "\n",
    "    all_documents = []\n",
    "    for path in pdf_paths:\n",
    "        print(f\"Carregando: {path}\")\n",
    "        try:\n",
    "            text = extract_text_with_ocr(path)\n",
    "            filename = os.path.basename(path)\n",
    "            chunks = split_text_into_chunks(text, chunk_size, overlap)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                document = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": filename, \"chunk_index\": i}\n",
    "                )\n",
    "                all_documents.append(document)\n",
    "            print(f\"{len(chunks)} chunks criados para {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {path}: {e}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        raise ValueError(\"Nenhum documento foi carregado. Verifique os caminhos dos PDFs.\")\n",
    "    \n",
    "    print(f\"Total de chunks carregados: {len(all_documents)}\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vectorstore_with_chunks(pdf_paths, chunk_size=1000, overlap=200):\n",
    "\n",
    "    documents = load_pdf_documents_with_chunks(pdf_paths, chunk_size, overlap)\n",
    "    vectorstore = create_vectorstore(documents)\n",
    "    print(\"Vectorstore criado com chunks.\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar documentos, dividir em chunks e criar vetorstore\n",
    "pdf_files = [\"data/lgpd.pdf\", \"data/codigo_civil.pdf\"]\n",
    "vectorstore_with_chunks = prepare_vectorstore_with_chunks(pdf_files, chunk_size=1000, overlap=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reexecução de Perguntas Após a Divisão em Chunks\n",
    "\n",
    "## Objetivo\n",
    "Após implementar a divisão dos documentos em chunks, vamos refazer as mesmas perguntas para avaliar se:\n",
    "1. A recuperação de contexto está mais precisa.\n",
    "2. Fontes irrelevantes, como o **Código Civil**, não estão mais sendo consultadas em perguntas sobre a **LGPD**.\n",
    "3. As respostas geradas pelo modelo são mais relevantes e contextualizadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodologia\n",
    "1. **Pipeline Atualizada**: Utilizar a pipeline com o vetorstore criado a partir dos documentos divididos em chunks.\n",
    "2. **Análise das Respostas**:\n",
    "   - Verificar se os chunks recuperados pertencem às fontes esperadas.\n",
    "   - Avaliar se as respostas geradas estão em conformidade com a pergunta.\n",
    "3. **Registro das Fontes**:\n",
    "   - Analisar as fontes consultadas para garantir que apenas documentos relevantes foram utilizados.\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados Esperados\n",
    "1. Respostas mais precisas e contextualizadas, diretamente relacionadas aos documentos corretos.\n",
    "2. Consultas à **LGPD** limitadas aos chunks de `lgpd.pdf`, eliminando interferências de fontes irrelevantes como `codigo_civil.pdf`.\n",
    "3. Melhor rastreabilidade das fontes devido aos metadados incluídos em cada chunk.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer uma pergunta\n",
    "query1 = \"definicao lei geral de protecao de dados\"\n",
    "response1 = ask_question(query1, vectorstore_with_chunks)\n",
    "\n",
    "print(\"Pergunta:\", response1[\"question\"])\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response1[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"capitulo das perdas e danos?\"\n",
    "response2 = ask_question(query2, vectorstore_with_chunks)\n",
    "\n",
    "print(\"Pergunta:\", response2[\"question\"])\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response2[\"response\"])\n",
    "\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response2[\"sources\"]):\n",
    "    print(\"-\", source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados Após a Implementação dos Chunks\n",
    "\n",
    "## Observação\n",
    "Após dividir os documentos em **chunks**, refizemos as perguntas utilizando a nova pipeline. Com essa abordagem, conseguimos:\n",
    "\n",
    "1. **Recuperar as Fontes Corretamente**:\n",
    "   - As perguntas sobre a **LGPD** consultaram apenas os chunks relevantes de `lgpd.pdf`.\n",
    "   - Fontes irrelevantes, como o `codigo_civil.pdf`, não foram mais incluídas no contexto.\n",
    "\n",
    "2. **Respostas Mais Assertivas**:\n",
    "   - As respostas geradas pelo modelo foram mais precisas e alinhadas às perguntas feitas.\n",
    "   - A relevância e o contexto melhoraram significativamente devido ao uso de chunks menores e bem definidos.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefícios Identificados\n",
    "1. **Melhor Foco no Contexto**:\n",
    "   - O sistema agora prioriza trechos específicos dos documentos, reduzindo a interferência de fontes não relacionadas.\n",
    "2. **Aumento na Precisão**:\n",
    "   - Respostas mais confiáveis e fundamentadas, com base em documentos diretamente relevantes ao tema da pergunta.\n",
    "3. **Rastreabilidade**:\n",
    "   - Cada resposta pode ser associada aos chunks e documentos específicos usados para gerar o contexto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de Histórico na Interação com o Modelo\n",
    "\n",
    "## Objetivo\n",
    "Adicionar um histórico de conversas para que o modelo possa basear suas respostas em interações anteriores, fornecendo um contexto mais rico e um feedback mais assertivo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_history_with_previous(history):\n",
    "    \"\"\"\n",
    "    Inclua todo o histórico, destacando explicitamente a interação mais recente.\n",
    "    \"\"\"\n",
    "    if not history:\n",
    "        return \"Histórico vazio.\"\n",
    "\n",
    "    # Última interação (pergunta e resposta anterior)\n",
    "    last_interaction = f\"Pergunta Anterior: {history[-1][0]}\\nResposta Anterior: {history[-1][1]}\"\n",
    "    \n",
    "    # Outras interações (exceto a última)\n",
    "    previous_interactions = \"\\n\".join([\n",
    "        f\"Pergunta: {q}\\nResposta: {r}\" for q, r in history[:-1]\n",
    "    ])\n",
    "    \n",
    "    # Combinar tudo\n",
    "    return f\"{previous_interactions}\\n\\n{last_interaction}\" if previous_interactions else last_interaction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_with_history(query, context, history):\n",
    "\n",
    "    # Incluir o histórico completo\n",
    "    full_history = summarize_history_with_previous(history)\n",
    "    \n",
    "    # Destacar a última interação diretamente no contexto\n",
    "    if history:\n",
    "        last_question, last_answer = history[-1]\n",
    "        last_interaction_context = f\"Baseando-se na última interação:\\nPergunta: {last_question}\\nResposta: {last_answer}\\n\"\n",
    "    else:\n",
    "        last_interaction_context = \"\"\n",
    "\n",
    "    # Construir o prompt\n",
    "    prompt = f\"\"\"\n",
    "    Você é um assistente jurídico especializado no Código Civil brasileiro.\n",
    "\n",
    "    Última Interação:\n",
    "    {last_interaction_context}\n",
    "\n",
    "    Histórico de Conversa:\n",
    "    {full_history}\n",
    "\n",
    "    Trechos relevantes dos documentos:\n",
    "    {context}\n",
    "\n",
    "    Nova Pergunta:\n",
    "    {query}\n",
    "\n",
    "    Resposta:\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response_with_history(query, vectorstore, history):\n",
    "\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "    \n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Fonte: {doc.metadata.get('source', 'Desconhecido')}\\nTrecho: {doc.page_content[:300]}\" for doc in docs\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    prompt = format_prompt_with_history(query, context, history)\n",
    "    \n",
    "    response = llm.invoke([prompt])\n",
    "    \n",
    "    history.append((query, response.content.strip()))\n",
    "    \n",
    "    output = {\n",
    "        \"question\": query,\n",
    "        \"response\": response.content.strip(),\n",
    "        \"sources\": [doc.metadata.get(\"source\", \"Desconhecido\") for doc in docs],\n",
    "        \"context_used\": context  \n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_question_with_history(query, vectorstore, history):\n",
    "\n",
    "    return generate_response_with_history(query, vectorstore, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "response1 = ask_question_with_history(\n",
    "    \"Quais são os capítulos e seções do Livro II do Código Civil?\", \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "print(response1[\"context_used\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response1[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Resposta\n",
    "A resposta gerada está **incorreta**. Os capítulos e seções apresentados não correspondem ao conteúdo real do **Livro II do Código Civil brasileiro**.\n",
    "\n",
    "---\n",
    "\n",
    "## Próxima Ação: Passar a Definição Correta no Próximo Input\n",
    "Para corrigir o erro, vamos:\n",
    "1. **Inserir a definição correta** dos capítulos e seções do Livro II como parte do próximo input.\n",
    "2. **Atualizar o histórico** para que o modelo possa melhorar sua resposta com base no contexto correto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = ask_question_with_history(\n",
    "    '''Os capítulos do Livro II do Código Civil são Livro II – Dos Bens\n",
    "Título Único – Das Diferentes Classes de Bens\n",
    "\n",
    "Capítulo I – Dos Bens Considerados em Si Mesmos\n",
    "Seção I – Dos Bens Imóveis\n",
    "Seção II – Dos Bens Móveis\n",
    "Seção III – Dos Bens Fungíveis e Consumíveis\n",
    "Seção IV – Dos Bens Divisíveis\n",
    "Seção V – Dos Bens Singulares e Coletivos\n",
    "Capítulo II – Dos Bens Reciprocamente Considerados\n",
    "Capítulo III – Dos Bens Públicos''', \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response2[\"response\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response2[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reexecução da Pergunta com Feedback Fornecido\n",
    "\n",
    "## Contexto\n",
    "Após identificar que a resposta gerada anteriormente estava **incorreta**, fornecemos ao modelo o feedback necessário com a definição correta dos capítulos e seções do **Livro II do Código Civil**.\n",
    "\n",
    "---\n",
    "\n",
    "## Próxima Ação\n",
    "Agora, faremos a mesma pergunta novamente para verificar se o modelo:\n",
    "1. **Incorporou o feedback fornecido**.\n",
    "2. **Gera uma resposta correta** baseada no novo contexto.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Próximos Passos\n",
    "1. **Executar a pergunta novamente**.\n",
    "2. **Analisar a resposta gerada** para validar se ela corresponde ao contexto fornecido.\n",
    "3. Documentar os resultados e, se necessário, continuar ajustando o modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = ask_question_with_history(\n",
    "    \"Quais são os capítulos e seções do Livro II do Código Civil?\", \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response3[\"response\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response3[\"sources\"]):\n",
    "    print(\"-\", source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso do Histórico e Introdução ao Cross-Encoder\n",
    "\n",
    "## 1. Histórico Utilizado com Sucesso\n",
    "O uso do histórico na interação demonstrou ser eficaz para melhorar a precisão das respostas. Após fornecer o feedback necessário ao modelo e reutilizar o histórico, conseguimos obter a **resposta correta** para a pergunta:\n",
    "\n",
    "**Pergunta**: *\"Quais são os capítulos e seções do Livro II do Código Civil?\"*\n",
    "\n",
    "**Resposta Gerada**:  \n",
    "- **Livro II – Dos Bens**  \n",
    "- **Título Único – Das Diferentes Classes de Bens**\n",
    "\n",
    "### Benefícios do Histórico\n",
    "1. **Aprendizado Contínuo**: O modelo ajustou suas respostas com base no feedback e nas interações anteriores.\n",
    "2. **Melhoria na Precisão**: As respostas passaram a refletir melhor o contexto fornecido.\n",
    "3. **Contexto Rico**: O histórico permitiu ao modelo considerar informações adicionais e construir respostas mais completas.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hipótese: Uso de Cross-Encoder para Melhorar a Rastreabilidade\n",
    "Podemos explorar o uso de um **cross-encoder** para refinar ainda mais o sistema. \n",
    "\n",
    "### O que é um Cross-Encoder?\n",
    "O cross-encoder é uma técnica de aprendizado profundo que:\n",
    "1. **Atribui pontuações de similaridade** diretamente entre a consulta e os trechos de texto recuperados.\n",
    "2. **Reavalia o contexto de maneira mais precisa** ao considerar todas as interações entre o texto e a consulta.\n",
    "\n",
    "### Benefícios Esperados do Cross-Encoder\n",
    "1. **Melhor Rastreabilidade**:\n",
    "   - O cross-encoder pode identificar com maior precisão os trechos mais relevantes, permitindo que o modelo utilize apenas os dados mais confiáveis.\n",
    "2. **Contexto Otimizado**:\n",
    "   - Refinando a seleção de trechos, o modelo terá um contexto mais relevante para gerar respostas.\n",
    "3. **Respostas Mais Precisas**:\n",
    "   - O uso de pontuações de similaridade permite ao sistema priorizar trechos que respondam diretamente à consulta, minimizando a interferência de informações secundárias.\n",
    "\n",
    "### Próximos Passos\n",
    "1. **Implementar o Cross-Encoder**:\n",
    "   - Integrar o modelo de cross-encoder para reavaliar os trechos recuperados antes de passá-los ao modelo.\n",
    "2. **Validar Hipótese**:\n",
    "   - Testar o impacto do cross-encoder na rastreabilidade e precisão das respostas.\n",
    "3. **Comparação com Resultados Anteriores**:\n",
    "   - Analisar as melhorias introduzidas pelo cross-encoder em relação ao uso do histórico sozinho.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "O uso do histórico melhorou significativamente as respostas do modelo, mas a introdução de um cross-encoder pode ser uma solução adicional para refinar ainda mais a rastreabilidade e precisão do sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "def rerank_documents_with_crossencoder(query: str, docs: List, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\") -> List:\n",
    "    \"\"\"\n",
    "    Reorganiza os documentos usando um modelo CrossEncoder com base na relevância para a consulta.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    # Inicializar o modelo CrossEncoder\n",
    "    cross_encoder = CrossEncoder(model_name)\n",
    "\n",
    "    # Criar pares de (query, document) para o modelo\n",
    "    query_doc_pairs = [(query, doc.page_content) for doc in docs]\n",
    "\n",
    "    # Obter scores de relevância\n",
    "    scores = cross_encoder.predict(query_doc_pairs)\n",
    "\n",
    "    # Ordenar os documentos pelo score em ordem decrescente\n",
    "    sorted_docs = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "\n",
    "def generate_response_with_rerank(query: str, vectorstore, history: List) -> dict:\n",
    "    \"\"\"\n",
    "    Gera uma resposta utilizando re-ranking dos documentos mais relevantes para a consulta.\n",
    "    \"\"\"\n",
    "    # Buscar mais documentos relevantes no Vectorstore\n",
    "    initial_docs = vectorstore.similarity_search(query, k=10)  # Aumentado para considerar mais documentos\n",
    "\n",
    "    if not initial_docs:\n",
    "        return {\n",
    "            \"question\": query,\n",
    "            \"response\": \"Nenhum documento relevante encontrado para a consulta.\",\n",
    "            \"sources\": [],\n",
    "            \"context_used\": \"\"\n",
    "        }\n",
    "\n",
    "    # Aplicar re-ranking nos documentos retornados\n",
    "    reranked_docs = rerank_documents_with_crossencoder(query, initial_docs)\n",
    "\n",
    "    # Selecionar os top 5 documentos após o re-ranking\n",
    "  \n",
    "\n",
    "    # Construir o contexto a partir dos documentos selecionados\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Fonte: {doc.metadata.get('source', 'Desconhecido')}\\nTrecho: {doc.page_content}\" for doc in reranked_docs\n",
    "    ])\n",
    "\n",
    "    # Gerar o prompt com o histórico e contexto\n",
    "    #prompt \n",
    "\n",
    "    prompt = format_prompt_with_history(query, context, history)\n",
    "    # Obter a resposta do modelo LLM\n",
    "    response = llm.invoke([prompt])\n",
    "\n",
    "    # Adicionar a interação ao histórico\n",
    "    history.append((query, response.content.strip()))\n",
    "\n",
    "    # Formatar a saída\n",
    "    output = {\n",
    "        \"question\": query,\n",
    "        \"response\": response.content.strip(),\n",
    "        \"sources\": [doc.metadata.get(\"source\", \"Desconhecido\") for doc in reranked_docs],\n",
    "        \"context_used\": context\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def ask_question_with_rerank(query: str, vectorstore, history: List) -> dict:\n",
    "    \"\"\"\n",
    "    Wrapper para gerar respostas com re-ranking utilizando o CrossEncoder.\n",
    "    \"\"\"\n",
    "    return generate_response_with_rerank(query, vectorstore, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "response1 = ask_question_with_rerank(\n",
    "    \"Quais são os capítulos e seções do Livro II do Código Civil brasileiro?\", \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "print(response1[\"context_used\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response1[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados do Uso de Rerank na Recuperação de Contexto\n",
    "\n",
    "## Observação\n",
    "Com a introdução do **rerank** no pipeline. A técnica conseguiu priorizar trechos que mencionam mais diretamente a **keyword \"Livro II\"**, resultando em um contexto mais relevante para a pergunta.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Limitações Identificadas\n",
    "Apesar da melhoria no contexto recuperado, a **resposta gerada pelo modelo ainda está incorreta**. Isso pode indicar:\n",
    "1. **Interpretação Limitada do Modelo**:\n",
    "   - O modelo não está aproveitando plenamente o contexto priorizado pelo rerank.\n",
    "2. **Necessidade de Ajustes no Contexto**:\n",
    "   - O contexto, embora relevante, pode não estar detalhado o suficiente para auxiliar o modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = ask_question_with_rerank(\n",
    "    '''Os capítulos do Livro II do Código Civil são Livro II – Dos Bens\n",
    "Título Único – Das Diferentes Classes de Bens\n",
    "\n",
    "Capítulo I – Dos Bens Considerados em Si Mesmos\n",
    "Seção I – Dos Bens Imóveis\n",
    "Seção II – Dos Bens Móveis\n",
    "Seção III – Dos Bens Fungíveis e Consumíveis\n",
    "Seção IV – Dos Bens Divisíveis\n",
    "Seção V – Dos Bens Singulares e Coletivos\n",
    "Capítulo II – Dos Bens Reciprocamente Considerados\n",
    "Capítulo III – Dos Bens Públicos''', \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response2[\"response\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response2[\"sources\"]):\n",
    "    print(\"-\", source)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = ask_question_with_rerank(\n",
    "    \"Quais são os capítulos e seções do Livro II do Código Civil?\", \n",
    "    vectorstore_with_chunks, \n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response3[\"response\"])\n",
    "print(\"\\nFontes:\")\n",
    "for source in set(response3[\"sources\"]):\n",
    "    print(\"-\", source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise: Histórico e Rerank\n",
    "\n",
    "## Observação Geral\n",
    "Embora tenhamos introduzido o uso do histórico e do rerank para melhorar a recuperação de contexto e a precisão das respostas, o modelo apresentou limitações em interações mais longas e iterativas. Na terceira pergunta, mesmo após o feedback fornecido, o modelo **não conseguiu encontrar a resposta correta**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Uso de Metadados na Recuperação de Documentos\n",
    "\n",
    "## Objetivo\n",
    "Melhorar a rastreabilidade e precisão na recuperação de informações ao:\n",
    "1. **Incluir metadados** como Livro, Capítulo e Seção em cada chunk extraído dos documentos.\n",
    "2. Estruturar os chunks de forma mais granular, associando cada parte do texto aos seus respectivos metadados.\n",
    "\n",
    "---\n",
    "## Hipotese\n",
    "1. Preparar um vetorstore utilizando documentos com metadados para consultas mais precisas.\n",
    "---\n",
    "\n",
    "## Beneficio\n",
    "1. Facilitar a rastreabilidade, associando cada chunk aos metadados contextuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_chunks_v2(text, chunk_size=1000, overlap=200):\n",
    "\n",
    "    # Padrões para detectar Livro, Capítulo e Seção (case-insensitive)\n",
    "    livro_pattern = re.compile(r\"(Livro\\s+[IVXLCDM\\d]+\\s*.*?)\\s*(?=\\n|$)\", re.IGNORECASE)\n",
    "    capitulo_pattern = re.compile(r\"(Capítulo\\s+[IVXLCDM\\d]+\\s*.*?)\\s*(?=\\n|$)\", re.IGNORECASE)\n",
    "    secao_pattern = re.compile(r\"(Seção\\s+[IVXLCDM\\d]+\\s*.*?)\\s*(?=\\n|$)\", re.IGNORECASE)\n",
    "\n",
    "    # Variáveis para acompanhar os metadados atuais\n",
    "    current_livro = None\n",
    "    current_capitulo = None\n",
    "    current_secao = None\n",
    "\n",
    "    # Lista de chunks e variáveis de controle\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    def save_chunk_and_reset():\n",
    "        nonlocal current_chunk, current_size\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"content\": \"\\n\".join(current_chunk),\n",
    "                \"metadata\": {\n",
    "                    \"livro\": current_livro,\n",
    "                    \"capítulo\": current_capitulo,\n",
    "                    \"seção\": current_secao\n",
    "                }\n",
    "            })\n",
    "            # Reiniciar mantendo o overlap\n",
    "            current_chunk = current_chunk[-overlap:]\n",
    "            current_size = sum(len(line) for line in current_chunk)\n",
    "\n",
    "    # Processar linhas do texto\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # Detectar mudanças nos metadados\n",
    "        if livro_match := livro_pattern.match(line):\n",
    "            save_chunk_and_reset()\n",
    "            current_livro = livro_match.group(1)\n",
    "\n",
    "        if capitulo_match := capitulo_pattern.match(line):\n",
    "            save_chunk_and_reset()\n",
    "            current_capitulo = capitulo_match.group(1)\n",
    "\n",
    "        if secao_match := secao_pattern.match(line):\n",
    "            save_chunk_and_reset()\n",
    "            current_secao = secao_match.group(1)\n",
    "\n",
    "        # Adicionar linha ao chunk atual\n",
    "        current_chunk.append(line)\n",
    "        current_size += len(line)\n",
    "\n",
    "        # Criar chunk se atingir o tamanho definido\n",
    "        if current_size >= chunk_size:\n",
    "            save_chunk_and_reset()\n",
    "\n",
    "    # Adicionar o último chunk\n",
    "    save_chunk_and_reset()\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_pdf_documents_v2(pdf_paths, chunk_size=1000, overlap=200):\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            # Abrir o PDF\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                print(f\"Processando arquivo: {pdf_path}\")\n",
    "                for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                    # Extrair texto da página\n",
    "                    text = page.extract_text()\n",
    "                    \n",
    "                    if text:\n",
    "\n",
    "\n",
    "                        # Dividir o texto em chunks\n",
    "                        chunks = split_chunks_v2(text, chunk_size, overlap)\n",
    "                        \n",
    "                        if not chunks:\n",
    "                            continue\n",
    "                        \n",
    "                        # Adicionar chunks à lista de documentos\n",
    "                        for chunk in chunks:\n",
    "                            documents.append(Document(\n",
    "                                page_content=chunk[\"content\"],\n",
    "                                metadata={\n",
    "                                    \"source\": pdf_path,\n",
    "                                    \"page\": page_number,\n",
    "                                    **chunk[\"metadata\"]  # Inclui metadados como Livro, Capítulo, Seção\n",
    "                                }\n",
    "                            ))\n",
    "                    else:\n",
    "                            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o arquivo {pdf_path}: {e}\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"Nenhum documento foi carregado. Verifique os arquivos PDF ou os métodos de extração.\")\n",
    "    else:\n",
    "        print(f\"Carregados {len(documents)} documentos do total de {len(pdf_paths)} arquivos PDF.\")\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vectorstore_with_chunks_v2(pdf_paths, chunk_size=2000, overlap=300):\n",
    "    # Carregar documentos estruturados com resumos\n",
    "    documents = load_pdf_documents_v2(pdf_paths, chunk_size, overlap)\n",
    "\n",
    "    # Criar vetorstore\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    return [vectorstore, documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_with_chunks_v, documents = prepare_vectorstore_with_chunks_v2(pdf_files, chunk_size=2000, overlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ask_question_with_history_v2(question, vectorstore, conversation_history):\n",
    "\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chains.question_answering import load_qa_chain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True  # Garante que as fontes sejam retornadas\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question, \"chat_history\": conversation_history})\n",
    "\n",
    "    conversation_history.append((question, result[\"answer\"]))\n",
    "\n",
    "    metadata_list = []\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        metadata = doc.metadata\n",
    "        metadata_list.append({\n",
    "            \"livro\": metadata.get(\"livro\"),\n",
    "            \"capítulo\": metadata.get(\"capítulo\"),\n",
    "            \"seção\": metadata.get(\"seção\"),\n",
    "            \"content\": doc.page_content  \n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"response\": result[\"answer\"],\n",
    "        \"context_used\": metadata_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "conversation_history = []\n",
    "response1 = ask_question_with_history_v2(\n",
    "    \"Perdas e danos\",\n",
    "    vectorstore_with_chunks_v,\n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "# Exibir resposta e contexto\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# print(\"Contexto utilizado:\")\n",
    "# for context in response1[\"context_used\"]:\n",
    "#     print(f\"Livro: {context['livro']}\")\n",
    "#     print(f\"Capítulo: {context['capítulo']}\")\n",
    "#     print(f\"Seção: {context['seção']}\")\n",
    "#     print(\"Conteúdo:\\n\", context[\"content\"])\n",
    "#     print(\"\\n---\\n\")\n",
    "print(\"Primeiro Contexto Utilizado:\")\n",
    "if response1[\"context_used\"]:\n",
    "    first_context = response1[\"context_used\"][0]\n",
    "    print(f\"Livro: {first_context['livro']}\")\n",
    "    print(f\"Capítulo: {first_context['capítulo']}\")\n",
    "    print(f\"Seção: {first_context['seção']}\")\n",
    "    print(\"Conteúdo:\\n\", first_context[\"content\"])\n",
    "else:\n",
    "    print(\"Nenhum contexto foi utilizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Análise\n",
    "\n",
    "### Metadados\n",
    "- **Capítulo**: O modelo identificou corretamente o **Capítulo III – Das Perdas e Danos** como relevante.\n",
    "- **Livro e Seção**: Não foram detectados metadados específicos, mas o capítulo principal foi recuperado com sucesso.\n",
    "\n",
    "### Contexto\n",
    "- O conteúdo recuperado cita diretamente os artigos legais que tratam de perdas e danos.\n",
    "- A precisão do contexto alinhou-se à pergunta, proporcionando uma base sólida para a resposta.\n",
    "\n",
    "### Resposta\n",
    "- A resposta gerada foi bem detalhada e consistente com o contexto jurídico recuperado.\n",
    "- Incluiu explicações adicionais sobre perdas e danos, tipos e cálculo.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "O uso de metadados melhorou a **rastreamento** e a **relevância do contexto** recuperado. O modelo foi capaz de identificar o **Capítulo III – Das Perdas e Danos** e utilizá-lo para construir uma resposta clara e detalhada. Essa abordagem reforça a importância de estruturar documentos com metadados para consultas mais precisas e contextualizadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melhorando os metadados usando mais metadados de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Função para dividir o texto em chunks e gerar metadados com NMF\n",
    "def split_chunks_with_ml_metadata(text, chunk_size=2000, overlap=300, page_number=None, source=None):\n",
    "\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    current_size = 0\n",
    "    chunk_index = 0\n",
    "    line_start = 1\n",
    "\n",
    "    def save_chunk_and_reset(line_end):\n",
    "        nonlocal current_chunk, current_size, chunk_index, line_start\n",
    "        if current_chunk:\n",
    "            content = \"\\n\".join(current_chunk)\n",
    "            metadata = extract_topics_with_nmf(content)\n",
    "            metadata.update({\n",
    "                \"linha_inicio\": line_start,\n",
    "                \"linha_fim\": line_end,\n",
    "                \"pagina\": page_number,\n",
    "                \"source\": source\n",
    "            })\n",
    "            chunks.append({\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"content\": content,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            chunk_index += 1\n",
    "            line_start = line_end - len(current_chunk) + 1\n",
    "            current_chunk = current_chunk[-overlap:]\n",
    "            current_size = sum(len(line) for line in current_chunk)\n",
    "\n",
    "    for i, line in enumerate(text.split(\"\\n\"), start=1):\n",
    "        current_chunk.append(line)\n",
    "        current_size += len(line)\n",
    "\n",
    "        if current_size >= chunk_size:\n",
    "            save_chunk_and_reset(i)\n",
    "\n",
    "    save_chunk_and_reset(len(text.split(\"\\n\")))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Função para extrair tópicos com NMF\n",
    "def extract_topics_with_nmf(content, n_topics=2):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"portuguese\")   \n",
    "    X = vectorizer.fit_transform([content])\n",
    "\n",
    "    nmf = NMF(n_components=n_topics, random_state=42)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_terms = [terms[i] for i in topic.argsort()[:-5 - 1:-1]]\n",
    "        topics.append(\" \".join(top_terms))\n",
    "\n",
    "    return {\n",
    "        \"topico_1\": topics[0] if len(topics) > 0 else None,\n",
    "        \"topico_2\": topics[1] if len(topics) > 1 else None\n",
    "    }\n",
    "\n",
    "# Função para carregar PDFs e dividir em chunks\n",
    "def load_and_chunk_pdfs_with_ml_metadata(pdf_paths, chunk_size=2000, overlap=300):\n",
    "    documents = []\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        chunks = split_chunks_with_ml_metadata(\n",
    "                            text, chunk_size, overlap, page_number=page_number, source=pdf_path\n",
    "                        )\n",
    "                        for chunk in chunks:\n",
    "                            documents.append(Document(\n",
    "                                page_content=chunk[\"content\"],\n",
    "                                metadata=chunk[\"metadata\"]\n",
    "                            ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Função para criar o vetorstore\n",
    "def create_vectorstore_with_metadata(documents):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# Função principal para todo o fluxo\n",
    "def prepare_vectorstore_from_pdfs_with_ml(pdf_paths, chunk_size=2000, overlap=300):\n",
    "    documents = load_and_chunk_pdfs_with_ml_metadata(pdf_paths, chunk_size, overlap)\n",
    "    vectorstore = create_vectorstore_with_metadata(documents)\n",
    "    return [vectorstore, documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_ml, documents_ml = prepare_vectorstore_from_pdfs_with_ml(pdf_files, chunk_size=2000, overlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_with_history_v3(question, vectorstore, conversation_history):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True  # Garante que as fontes sejam retornadas\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"question\": question, \"chat_history\": conversation_history})\n",
    "\n",
    "    conversation_history.append((question, result[\"answer\"]))\n",
    "\n",
    "    metadata_list = []\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        metadata = doc.metadata\n",
    "        metadata_list.append({\n",
    "\n",
    "            \"topico_1\": metadata.get(\"topico_1\"),\n",
    "            \"topico_2\": metadata.get(\"topico_2\"),\n",
    "            \"content\": doc.page_content\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"response\": result[\"answer\"],\n",
    "        \"context_used\": metadata_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "conversation_history = []\n",
    "response1 = ask_question_with_history_v3(\n",
    "    \"Perdas e danos\",\n",
    "    vectorstore_ml,\n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "# Exibir resposta e contexto\n",
    "print(\"\\nResposta Gerada:\")\n",
    "print(response1[\"response\"])\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "for context in response1[\"context_used\"]:\n",
    "    print(f\"Tópico 1: {context.get('topico_1', 'Não encontrado')}\")\n",
    "    print(f\"Tópico 2: {context.get('topico_2', 'Não encontrado')}\")\n",
    "    print(\"Conteúdo:\")\n",
    "    print(context.get(\"content\", \"Conteúdo não encontrado\"))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não foi muito efetivo o uso de "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
